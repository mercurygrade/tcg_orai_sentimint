{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import twint \n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_filter(text: str) -> str:\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.search(regex, text)\n",
    "    \n",
    "    if url and len(url[0]) / len(text) > 0.50:\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scam_tweets(text: str) -> str:\n",
    "    word_black_list = [\"giving away\", \"Giving away\", \"GIVING AWAY\", \"PRE-GIVEAWAY\", \"Giveaway\", \"GIVEAWAY\", \"giveaway\", \"follow me\", \"Follow me\", \"FOLLOW ME\", \"retweet\", \"Retweet\", \"RETWEET\", \"LIKE\", \"airdrop\",\"AIRDROP\", \"Airdrop\", \"free\", \"FREE\", \"Free\", \"-follow\", \"-Follow\", \"-rt\", \"-Rt\", \"Requesting faucet funds\"]\n",
    "    if any(ext in text for ext in word_black_list):\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "\n",
    "    text = text.replace(\"\\n\", \"\") \n",
    "    text = url_filter(text)\n",
    "    text = filter_scam_tweets(text)\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    text = text.replace(\"#\", \"\") # remove hashtags from tweet\n",
    "    \n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, \"\", text) # remove URL's from tweet\n",
    "        \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_list(start_year: int, start_month: int, start_day: int, number_of_days: int) -> list[str]:\n",
    "    start_date = datetime.date(start_year, start_month, start_day)\n",
    "    date_list = []\n",
    "\n",
    "    for day in range(number_of_days):\n",
    "        date_str = (start_date + datetime.timedelta(days = day)).isoformat()\n",
    "        date_list.append(date_str)\n",
    "    \n",
    "    return date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(day: str, topic: str, num_of_tweets: int) -> pd.DataFrame:\n",
    "    config = twint.Config()\n",
    "\n",
    "    config.Search = topic # What topic should the tweets be about\n",
    "    config.Limit = num_of_tweets # How many tweets do we want to scrape\n",
    "    config.Lang = \"en\"\n",
    "    config.Since = f\"{day} 00:00:00\" # Specify the day \n",
    "    config.Until = f\"{day} 23:59:59\"\n",
    "\n",
    "    config.Pandas = True\n",
    "    config.Store_object = True\n",
    "    twint.run.Search(config)\n",
    "\n",
    "    df = twint.storage.panda.Tweets_df # Create a Pandas DataFrame of the scraped tweets\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_tweets(day=\"2022-01-25\", topic=\"bitcoin\", num_of_tweets=50)\n",
    "\n",
    "date_list = create_date_list(start_year=2022, start_month=1, start_day=18, number_of_days=7)\n",
    "df_list = []\n",
    "\n",
    "for day in date_list:\n",
    "    df = scrape_tweets(day=day, topic=\"bitcoin\", num_of_tweets=400)\n",
    "    if len(df.columns) != 0:\n",
    "        df[\"tweet\"] = df[\"tweet\"].apply(lambda row: clean_text(row))\n",
    "        df = df[df[\"tweet\"] != \"SPAM\"]\n",
    "        df.dropna(subset=[\"tweet\"], inplace=True)\n",
    "        df_list.append(df)\n",
    "\n",
    "\n",
    "merged_df = pd.concat(df_list)\n",
    "merged_df.to_csv(\"bitcoin.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
